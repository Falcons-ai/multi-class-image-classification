{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class PyTorch Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "#Load truncated images regardless\n",
    "Image.LOAD_TRUNCATED_IMAGES = True\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "%matplotlib inline\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "btch_sz = 256\n",
    "# Set image size for model training\n",
    "pic_size = 244\n",
    "main_directory = './data_directory'\n",
    "train_path = main_directory+'/train/'\n",
    "test_path = main_directory+'/validation/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting paths and assigning classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get class names from directories\n",
    "classes = sorted([f.name for f in os.scandir(test_path) if f.is_dir()])\n",
    "num_imgs = []\n",
    "train_folders = sorted(glob(train_path+'/*'))\n",
    "for path in train_folders:\n",
    "    num = len(glob(path+'/*'))\n",
    "    num_imgs.append(num)\n",
    "set_file_count = str(num_imgs[0])+\"_\"\n",
    "# Declare vars for Confusion matrix\n",
    "preds_var = []\n",
    "actual_var = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(TN, FP, FN, TP):\n",
    "    total = TN + FP + FN + TP\n",
    "    accuracy = (TP + TN) / total\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = dict(\n",
    "        batch_size = btch_sz,\n",
    "        learning_rate = 0.001,\n",
    "        epochs = EPOCHS,\n",
    "        lin1_size = 300,\n",
    "        lin2_size = 300,\n",
    "        activation = 'relu',\n",
    "        model = 'resnet50'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Pie chart visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "\n",
    "def count_images_in_subdirs(main_dir):\n",
    "    labels = []\n",
    "    counts = []\n",
    "\n",
    "    # Loop through each subdirectory in the main directory\n",
    "    for subdir in os.listdir(main_dir):\n",
    "        subdir_path = os.path.join(main_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Count the number of images in the subdirectory\n",
    "            image_files = natsorted(glob.glob(f\"{subdir_path}/*.jpg\"))\n",
    "            num_images = len(image_files)\n",
    "            labels.append(subdir)\n",
    "            counts.append(num_images)\n",
    "    \n",
    "    return labels, counts\n",
    "\n",
    "def generate_pie_chart(labels, counts, locf='Train'):\n",
    "    myexplode = [0.1] * len(labels)  # Adjust this list if you want specific slices to be exploded\n",
    "    \n",
    "    # Combine labels and counts for legend\n",
    "    legend_labels = [f\"{label} ({count})\" for label, count in zip(labels, counts)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12.8, 9.6))  # Make the chart 2x larger\n",
    "    ax.pie(counts, labels=labels, autopct='%1.1f%%',\n",
    "           colors=plt.cm.tab20.colors, explode=myexplode, shadow=True, startangle=90)\n",
    "    plt.legend(legend_labels, loc='upper right', title=locf + \" Image Count\")\n",
    "    plt.title(f\"{locf} - Image Distribution\")\n",
    "    plt.savefig('torch_'+locf+'_dataset_pie.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = count_images_in_subdirs(train_path)\n",
    "generate_pie_chart(labels, counts, locf='Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = count_images_in_subdirs(test_path)\n",
    "generate_pie_chart(labels, counts, locf='Validation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.8,1.2), shear=5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_path, train_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_path, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CFG['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Random Training set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loader = DataLoader(train_dataset, batch_size=CFG['batch_size'], shuffle=True)\n",
    "\n",
    "# Visualise some examples\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    batch = next(iter(plot_loader))\n",
    "    label = int(batch[1].numpy()[0])\n",
    "    image = np.transpose(batch[0][0].numpy(), (1, 2, 0))/2\n",
    "    plt.imshow(image)\n",
    "    plt.title(classes[label])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and display state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "# Freeze the layers of the ResNet50 model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add a new classification head to the model\n",
    "model.fc = nn.Sequential(nn.Linear(2048, CFG['lin1_size']),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(CFG['lin2_size'], len(classes))).to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for layer in model.state_dict():\n",
    "    print(layer, \"\\t\", model.state_dict()[layer].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for state_var in optimizer.state_dict():\n",
    "    print(state_var, \"\\t\", optimizer.state_dict()[state_var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Model.state.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(model.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set extra model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.fc.parameters())\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['epochs'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, criterion, optimizer, scheduler):\n",
    "    global preds_var \n",
    "    global actual_var \n",
    "    print(\"Training...\")\n",
    "    # Train mode\n",
    "    model.train()\n",
    "    # Track metrics\n",
    "    loss_epoch = 0\n",
    "    accuracy_epoch = 0\n",
    "    # Loop over minibatches\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        # Send to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Track loss\n",
    "        loss_epoch += loss.detach().item()\n",
    "        # Accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        accuracy_epoch += torch.sum(preds == labels)/inputs.shape[0]\n",
    "\n",
    "        preds_var += preds.tolist()\n",
    "        actual_var += labels.tolist()\n",
    "        \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "        \n",
    "    return loss_epoch/len(train_loader), accuracy_epoch.item()/len(train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(test_loader, model, criterion):\n",
    "    print(\"Evaluation...\")\n",
    "    # Eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Track metrics\n",
    "    loss_epoch = 0\n",
    "    accuracy_epoch = 0\n",
    "    \n",
    "    # Don't update weights\n",
    "    with torch.no_grad():\n",
    "        # Loop over minibatches\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            # Send to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track loss\n",
    "            loss_epoch += loss.detach().item()\n",
    "            \n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            accuracy_epoch += torch.sum(preds == labels)/inputs.shape[0]\n",
    "    \n",
    "    return loss_epoch/len(test_loader), accuracy_epoch.item()/len(test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "def plot_hist(train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist):    \n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_loss_hist, label='Train_Loss')\n",
    "    plt.plot(test_loss_hist, label='Validation_loss')\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_acc_hist, label='Train_Accuracy')\n",
    "    plt.plot(test_acc_hist, label='Validation_Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"torch_performance.png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function to call train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, verbose=True):\n",
    "    # Initialise outputs\n",
    "    train_loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(CFG['epochs']):\n",
    "        # Train\n",
    "        train_loss, train_accuracy = train_one_epoch(train_loader, model, criterion, optimizer, scheduler)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_accuracy = evaluate_one_epoch(test_loader, model, criterion)\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss_hist.append(train_loss)\n",
    "        test_loss_hist.append(test_loss)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        \n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), \"model/chkpt_torch_model.pth\")\n",
    "        # Save model\n",
    "        torch.save(model, \"model/chkpt_torch_model-full.pth\")\n",
    "        print(\"Checkpoints saved\")\n",
    "        \n",
    "        # Print loss\n",
    "        if verbose:\n",
    "            if (epoch+1)%1==0:\n",
    "                print(f'Epoch {epoch+1}/{CFG[\"epochs\"]}, loss {train_loss:.5f}, test_loss {test_loss:.5f}, accuracy {train_accuracy:.5f}, test_accuracy {test_accuracy:.5f}')\n",
    "    \n",
    "    return train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training for \",train_path)\n",
    "# Train model\n",
    "train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_model(model, criterion, optimizer, \n",
    "                                                                             scheduler, train_loader, test_loader,  verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model/torch_model.pth\")\n",
    "# Save model\n",
    "torch.save(model, \"model/torch_model-full.pth\")\n",
    "# Remove Checkpoints\n",
    "os.remove(\"model/chkpt_torch_model-full.pth\")\n",
    "os.remove(\"model/chkpt_torch_model.pth\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for key,value in model._modules.items():\n",
    "    features.append(value)\n",
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def calculate_scores(cm):\n",
    "    correct_scores = [cm[i, i] / np.sum(cm[i]) for i in range(len(cm))]\n",
    "    return correct_scores\n",
    "\n",
    "def plot_confusion_matrix(actual, preds, class_labels):\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(actual, preds)\n",
    "    num_classes = len(class_labels)\n",
    "    \n",
    "    # Calculate scores for each label\n",
    "    scores = calculate_scores(cm)\n",
    "    \n",
    "    # Create a custom figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    \n",
    "    # Use a gradient color map for intensity\n",
    "    cmap = plt.cm.RdYlGn\n",
    "    \n",
    "    # Plot the confusion matrix with intensity proportional to scores\n",
    "    cax = ax.matshow(cm, cmap=cmap)\n",
    "    \n",
    "    # Set text color based on correct or incorrect predictions\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            color = 'white' if i == j else 'black'\n",
    "            ax.text(j, i, str(cm[i, j]), va='center', ha='center', color=color)\n",
    "    \n",
    "    # Set labels for the axes\n",
    "    ax.set_xticks(np.arange(num_classes))\n",
    "    ax.set_yticks(np.arange(num_classes))\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    def calculate_accuracy(cm):\n",
    "        return np.trace(cm) / np.sum(cm)\n",
    "    \n",
    "    accuracy = calculate_accuracy(cm)\n",
    "    str_title = f\"Confusion Matrix\\n{accuracy * 100:.2f}% accuracy.\"\n",
    "    plt.title(str_title)\n",
    "    \n",
    "    # Create custom legend handles and labels with scores\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], color=cmap(score), lw=4, label=f'{class_labels[i]}: {score:.2f}')\n",
    "        for i, score in enumerate(scores)\n",
    "    ]\n",
    "    \n",
    "    # Sort the legend handles based on scores\n",
    "    legend_handles = sorted(legend_handles, key=lambda x: float(x.get_label().split(': ')[1]), reverse=True)\n",
    "    \n",
    "    # Add a legend\n",
    "    legend = plt.legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(1, 1), title=\"Legend\")\n",
    "    plt.setp(legend.get_texts(), color='black')  # Set legend text color to black\n",
    "    \n",
    "    # Save CM without cropping the legend\n",
    "    plt.savefig(\"torch_confusion_matrix.png\", bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Assuming `model`, `test_loader`, and `classes` are defined elsewhere\n",
    "# Get predictions and actual labels from the test set\n",
    "actual_labels, predicted_labels = get_predictions(model, test_loader)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(actual_labels, predicted_labels, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Explanation\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "\n",
    "## Structure of the Confusion Matrix\n",
    "\n",
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It not only gives you insight into the errors being made by your classifier but also more importantly the types of errors that are being made.\n",
    "\n",
    "### Components of the Confusion Matrix\n",
    "\n",
    "For a binary classification problem, the confusion matrix looks like this:\n",
    "\n",
    "|                    | Predicted Negative | Predicted Positive |\n",
    "|--------------------|--------------------|--------------------|\n",
    "| **Actual Negative**| True Negative (TN) | False Positive (FP)|\n",
    "| **Actual Positive**| False Negative (FN)| True Positive (TP) |\n",
    "\n",
    "- **True Positive (TP):** The model correctly predicted the positive class.\n",
    "- **True Negative (TN):** The model correctly predicted the negative class.\n",
    "- **False Positive (FP):** The model incorrectly predicted the positive class (Type I error).\n",
    "- **False Negative (FN):** The model incorrectly predicted the negative class (Type II error).\n",
    "\n",
    "For a multi-class classification problem, the matrix expands to include rows and columns for each class.\n",
    "\n",
    "### Accuracy Calculation\n",
    "\n",
    "Accuracy is one metric for evaluating classification models. It is the ratio of correctly predicted instances to the total instances:\n",
    "\n",
    "\\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\]\n",
    "\n",
    "In a multi-class scenario, accuracy is calculated as the trace of the confusion matrix divided by the sum of all elements in the matrix:\n",
    "\n",
    "\\[ \\text{Accuracy} = \\frac{\\sum_{i} \\text{cm}[i,i]}{\\sum_{i,j} \\text{cm}[i,j]} \\]\n",
    "\n",
    "Where \\(\\text{cm}[i,j]\\) is the element of the confusion matrix at row \\(i\\) and column \\(j\\).\n",
    "\n",
    "### Visualizing the Confusion Matrix\n",
    "\n",
    "A visual representation of the confusion matrix can provide more insight into the model’s performance:\n",
    "\n",
    "1. **Matrix Plot:** Each cell's color intensity corresponds to the number of instances in that cell, with darker shades representing higher counts.\n",
    "2. **Axes Labels:** The x-axis represents the predicted classes, and the y-axis represents the actual classes.\n",
    "3. **Annotations:** Each cell shows the count of instances for that actual-predicted class pair.\n",
    "\n",
    "\n",
    "- The diagonal cells represent the number of times the model correctly predicted each class.\n",
    "- Off-diagonal cells represent the number of times the model confused one class with another.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **High Diagonal Values:** Indicates that the model performs well for those classes.\n",
    "- **High Off-Diagonal Values:** Indicates areas where the model is confused and may need improvement.\n",
    "\n",
    "### Usage\n",
    "\n",
    "By analyzing the confusion matrix, you can:\n",
    "\n",
    "- Identify which classes are being predicted correctly and which are not.\n",
    "- Understand the types of errors your model is making.\n",
    "- Make informed decisions to improve your model, such as collecting more data for classes with high error rates or adjusting the model’s complexity.\n",
    "\n",
    "Understanding the confusion matrix is crucial for improving the performance of your classification models and ensuring they are reliable and accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model across a random sampling of test_dir images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(img_size=pic_size):\n",
    "    from PIL import Image\n",
    "    import math\n",
    "    import torch\n",
    "    from torchvision import models, transforms\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    subdirs = [os.path.join(test_path, d) for d in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, d))]\n",
    "    all_images = []\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        images = [os.path.join(subdir, f) for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f)) and f.lower().endswith(('.jpg'))]\n",
    "        all_images.extend([(subdir, img) for img in images])\n",
    "\n",
    "    img = str(random.choice(all_images)[1])\n",
    "    # Load the image\n",
    "    image_path = img\n",
    "    image = Image.open(image_path)\n",
    "    # Transform target image\n",
    "    custom_image_transformed = test_transforms(image)\n",
    "    pred_model = torch.load(\"model/torch_model-full.pth\") \n",
    "    #Load for inferrence\n",
    "    pred_model.eval()\n",
    "    # Move the input data to the GPU\n",
    "    input_data = custom_image_transformed.cuda()\n",
    "    # Perform computations on the GPU\n",
    "    output_data = pred_model(input_data.unsqueeze(0))\n",
    "    _, index = torch.max(output_data, 1)\n",
    "    percentage = torch.nn.functional.softmax(output_data, dim=1)[0] * 100\n",
    "    #print(percentage)\n",
    "    #print(math.ceil(index))\n",
    "    message  = str(round(percentage[index[0]].item(),2))+\"% confident, this is \"+str(classes[index[0]])\n",
    "    #message  = str(percentage[index[0]].item())+\"% confident, this is \"+str(classes[index[0]])\n",
    "    img = image.resize((pic_size,pic_size))\n",
    "    plt.imshow(img)\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and return only the classification from random image selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def only_class(image_file, classes,img_size=pic_size):\n",
    "    from PIL import Image\n",
    "    import math\n",
    "    import torch\n",
    "    from torchvision import models, transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    # Load the image\n",
    "    image_path = image_file\n",
    "    image = Image.open(image_path)\n",
    "    # Transform target image\n",
    "    custom_image_transformed = test_transforms(image)\n",
    "    pred_model = torch.load(\"model/torch_model-full.pth\") \n",
    "    #Load for inferrence\n",
    "    pred_model.eval()\n",
    "    # Move the input data to the GPU\n",
    "    input_data = custom_image_transformed.cuda()\n",
    "    # Perform computations on the GPU\n",
    "    output_data = pred_model(input_data.unsqueeze(0))\n",
    "    _, index = torch.max(output_data, 1)\n",
    "    return image,str(classes[index[0]])\n",
    "\n",
    "def find_random_image(root_dir, extensions=['.jpg']):\n",
    "    # List to hold paths of all images\n",
    "    image_paths = []\n",
    "\n",
    "    # Walk through the directory and subdirectories\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            # Check if the file extension is one of the image extensions\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    # If there are no images, return None\n",
    "    if not image_paths:\n",
    "        return None\n",
    "\n",
    "    # Return a random image path\n",
    "    \n",
    "    return random.choice(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "random_image_path = find_random_image(main_directory)\n",
    "print(random_image_path)\n",
    "only_class(random_image_path, classes,img_size=pic_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tensor",
   "language": "python",
   "name": "py-tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
